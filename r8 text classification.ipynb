{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from gensim.models import word2vec\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.read_csv(\"r8-train.csv\")\n",
    "test=pd.read_csv(\"r8-test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>edge</th>\n",
       "      <th>intent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>champion product approv stock split champion p...</td>\n",
       "      <td>champion product approv stock split champion p...</td>\n",
       "      <td>earn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>comput termin system cpml complet sale comput ...</td>\n",
       "      <td>comput termin system cpml complet sale comput ...</td>\n",
       "      <td>acq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cobanco inc cbco year net shr ct dlr net asset...</td>\n",
       "      <td>cobanco inc cbco year net shr ct dlr net asset...</td>\n",
       "      <td>earn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>intern inc qtr jan oper shr loss two ct profit...</td>\n",
       "      <td>intern inc qtr jan oper shr loss two ct profit...</td>\n",
       "      <td>earn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>brown forman inc bfd qtr net shr dlr ct net ml...</td>\n",
       "      <td>brown forman inc bfd qtr net shr dlr ct net ml...</td>\n",
       "      <td>earn</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  champion product approv stock split champion p...   \n",
       "1  comput termin system cpml complet sale comput ...   \n",
       "2  cobanco inc cbco year net shr ct dlr net asset...   \n",
       "3  intern inc qtr jan oper shr loss two ct profit...   \n",
       "4  brown forman inc bfd qtr net shr dlr ct net ml...   \n",
       "\n",
       "                                                edge intent  \n",
       "0  champion product approv stock split champion p...   earn  \n",
       "1  comput termin system cpml complet sale comput ...    acq  \n",
       "2  cobanco inc cbco year net shr ct dlr net asset...   earn  \n",
       "3  intern inc qtr jan oper shr loss two ct profit...   earn  \n",
       "4  brown forman inc bfd qtr net shr dlr ct net ml...   earn  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#for text files-reading data\n",
    "X,y=[],[]\n",
    "with open(train,'r') as infile:\n",
    "    for line in infile:\n",
    "        label,text = line.split(\"\\t\")\n",
    "        X.append(text.split())\n",
    "        y.append(label)\n",
    "X,y=np.array(X),np.array(y)\n",
    "print(\"total example %s\" % len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=train.drop([\"intent\"],axis=1)\n",
    "y=train[\"intent\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test=test.drop([\"intent\"],axis=1)\n",
    "y_test=test[\"intent\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total example of train 4937\n",
      "total example of test 2189\n"
     ]
    }
   ],
   "source": [
    "print(\"total example of train %s\" % len(y))\n",
    "print(\"total example of test %s\" % len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['acq', 'crude', 'earn', 'grain', 'interest', 'money-fx', 'ship',\n",
       "        'trade'], dtype=object),\n",
       " array([1425,  237, 2610,   32,  132,  178,   95,  228], dtype=int64))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#finding unique labels available with us\n",
    "np.unique(y,return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "these are the unique labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Navy bias classification\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x data all words are concatenated\n",
    "X_text=[\" \".join(val) for val in X]\n",
    "print(X_test[0]+\"\\n\")\n",
    "\n",
    "X_test_text=[\" \".join(val) for val in X_test]\n",
    "print(X_test_text[0]+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# converting text to vectors and removing stop words'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vect=CountVectorizer(stop_words='english',max_features=5000)   #5000 different features is dervied \n",
    "vect.fit(X_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying the vectorizer to our test and train sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_transformed=vect.transform(X_text)\n",
    "X_test_transformed=vect.transform(X_test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#printing the vocabularoy\n",
    "list(vect.vocabulory_.itms())[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERNOUILLI NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "bnb=BernoulliNB()\n",
    "bnb.fit(X_train_transformed,y)\n",
    "\n",
    "#predict class\n",
    "pred_train_ys=bnb.predict(X_train_transformed)\n",
    "pred_test_ys=bnb.predict(X_test_transformed)\n",
    "\n",
    "#accuracy\n",
    "print(\"Train accuracy\", accuracy_score(y,pred_train_ys))\n",
    "print(\"Test accuracy\", accuracy_score(y,pred_test_ys))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MULTINOMIAL NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "mnb=MultinomialNB()\n",
    "\n",
    "mnb.fit(X_train_transformed,y)\n",
    "\n",
    "#predict class\n",
    "pred_train_ys=bnb.predict(X_train_transformed)\n",
    "pred_test_ys=bnb.predict(X_test_transformed)\n",
    "\n",
    "#accuracy\n",
    "print(\"Train accuracy\", accuracy_score(y,pred_train_ys))\n",
    "print(\"Test accuracy\", accuracy_score(y,pred_test_ys))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multinomial has imporved the accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WORD EMBEDDING APPROACHES\n",
    "\n",
    "1.  Word2vec-train our own vectors\n",
    "\n",
    "2.  Glove-using pretrained vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.Glove**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-4-d127d3bcd4f7>:5: DeprecationWarning: Call to deprecated `glove2word2vec` (KeyedVectors.load_word2vec_format(.., binary=False, no_header=True) loads GLoVE text vectors.).\n",
      "  glove2word2vec(glove_input_file,word2vec_output_file)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(400000, 100)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "\n",
    "glove_input_file='glove.6B.100d.txt'\n",
    "word2vec_output_file='glove.6B.100d.w2vformat.txt'\n",
    "glove2word2vec(glove_input_file,word2vec_output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "glove_model=KeyedVectors.load_word2vec_format(\"glove.6B.100d.w2vformat.txt\",binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**training our own word vectors on the data(test and train)**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_comb=np.concatenate([X,X_test])\n",
    "len(X_comb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_comb[6000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v=word2vec.Word2Vec(X_comb,window=2,min_count=2,sg=1,size=200)\n",
    "w2v.most_similar(\"future\",topn=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.Sentence vectors by averaging the word vectors**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting singular word into sentence vector by averaging the word\n",
    "\n",
    "\n",
    "def sent_vec_w2v(sent):   #passing ever word vector\n",
    "    wv_res=np.zeros(w2v.vector_size)\n",
    "    ctr=1\n",
    "    for w in sent:\n",
    "        if w in w2v:\n",
    "            ctr+=1           #average of word vector\n",
    "            wv_res+=w2v[w]\n",
    "        wv_res=wv_res/ctr\n",
    "        return wv_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#passing the test and train data through the above function\n",
    "\n",
    "train_doc_vecs=[]\n",
    "for doc in X:\n",
    "    doc_words=[term for term in doc if term not in stop_words]\n",
    "    train_doc_vecs.append(sent_vec_w2v(doc_words))\n",
    "    \n",
    "test_doc_vec=[]\n",
    "for doc in X_test:\n",
    "    doc_words=[term for term in doc if term not in stop_words]\n",
    "    test_doc_vecs.append(sent_vec_w2v(doc_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Logistic regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logreg=LogisticRegression(penalty='l1',random_state=42,C=8)\n",
    "\n",
    "#fitting with training data and label\n",
    "logreg.fit(train_docs_vecs,y)\n",
    "\n",
    "#prediction on training and test data\n",
    "pred_train_ys=logreg.predict(train_doc_vecs)\n",
    "pred_test_ys=logreg.predict(test_doc_vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get same accuracy with reduced set of features in Mulinomial NB\n",
    "\n",
    "Hence word vectors performance is better than Multinomial NB:\n",
    "1.  Smaller model\n",
    "2.  Lower time for modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
